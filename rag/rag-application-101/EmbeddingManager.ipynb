{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khajum/ai-playground/blob/main/rag/rag-application-101/EmbeddingManager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipelines - Data Ingestion to Verctor DB Pipeline"
      ],
      "metadata": {
        "id": "8kr7TTwnDbCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pypdf if not already installed\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install pypdf\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "f_ra-ZZKDkh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "qu69PJ1gFvQu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Read all the PDFs inside directory './data/pdf/'\n",
        "def process_all_pdfs(pdf_directory):\n",
        "  # Process all the pdf files inside the directory\n",
        "  all_documents = []\n",
        "  pdf_dir = Path(pdf_directory)\n",
        "\n",
        "  # Find all the pdf files recursively\n",
        "  pdf_files = list(pdf_dir.glob('**/*.pdf'))\n",
        "\n",
        "  # print all the pdf files found in the directory\n",
        "  print(f\"Found {len(list(pdf_files))} pdf files in {pdf_directory}\")\n",
        "\n",
        "  for pdf_file in pdf_files:\n",
        "    print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "    try:\n",
        "      loader = PyPDFLoader(str(pdf_file))\n",
        "      documents = loader.load()\n",
        "\n",
        "      # Add source information to metadata\n",
        "      for doc in documents:\n",
        "        doc.metadata['source_file'] = str(pdf_file.name)\n",
        "        doc.metadata['file_type'] = \"pdf\"\n",
        "\n",
        "      all_documents.extend(documents)\n",
        "      print(f\" Processed {pdf_file.name} with {len(documents)} pages\")\n",
        "    except Exception as e:\n",
        "      print(f\" Error processing {pdf_file.name}: {e}\")\n",
        "\n",
        "  print(f\" Total documents processed: {len(all_documents)}\")\n",
        "  return all_documents\n",
        "\n",
        "# Process all the PDFs in the data directory\n",
        "# Call the corrected function\n",
        "all_pdf_document = process_all_pdfs(\"./data\")\n",
        "print(f\"Successfully processed and collected {len(all_pdf_document)} documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMiRQbPtGg6t",
        "outputId": "03fa313c-51f2-4957-beaf-31f0e70e00a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 pdf files in ./data\n",
            "\n",
            "Processing: Attention-in-ML.pdf\n",
            " Processed Attention-in-ML.pdf with 11 pages\n",
            " Total documents processed: 11\n",
            "Successfully processed and collected 11 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "5xShnjMWSw7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_document(documents, chunk_size=1000, chunk_overlap=200):\n",
        "  # split the documents into smaller chucks for better RAG performance\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size, chunk_overlap,\n",
        "      length_function=len,\n",
        "      separators = [\"\\n\", \"\\n\\n\", \" \", \"\"])\n",
        "  splits = text_splitter.split_documents(documents)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "2fwylAeARpiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all embeding and vectordb related packaged if not already installed\n",
        "!pip install sentence-transformers faiss-cpu chromadb"
      ],
      "metadata": {
        "id": "QrBMunmxtrPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "F1Pces0FvEQ1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "  \"\"\"Handles the document embeding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "  def __init__(self, embedding_model_name:str=\"all-MiniLM-L6-v2\"):\n",
        "    \"\"\"\n",
        "    Initialize the EmbeddingManager.\n",
        "\n",
        "    Args:\n",
        "      embedding_model_name (str): Name of the embedding model to use.\n",
        "    \"\"\"\n",
        "    self.embedding_model_name = embedding_model_name\n",
        "    self.model = None\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    \"\"\"\n",
        "    Load the SentenceTransformer embedding model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print(f\"Loading SentenceTransformer model: {self.embedding_model_name}\")\n",
        "      self.model = SentenceTransformer(self.embedding_model_name)\n",
        "      print(f\"Loaded SentenceTransformer model successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading embedding model: {e}\")\n",
        "      raise\n",
        "\n",
        "  def generate_embeddings(self, texts:List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "      texts (List[str]): List of texts to generate embeddings for.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray: Array of embeddings.\n",
        "    \"\"\"\n",
        "    if self.model is None:\n",
        "      self._load_model()\n",
        "    try:\n",
        "      print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "      embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "      print(f\"Successfully generated embeddings for {len(texts)} texts with shape: {embeddings.shape}\")\n",
        "      return embeddings\n",
        "    except Exception as e:\n",
        "      print(f\"Error generating embeddings: {e}\")\n",
        "      raise\n",
        "\n",
        "\n",
        "# Initialize the embedding manager\n",
        "embedding_manager = EmbeddingManager()\n",
        "embeddings = embedding_manager.generate_embeddings([doc.page_content for doc in all_pdf_document])\n"
      ],
      "metadata": {
        "id": "-RRnCqOmuXa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVG_ddP-vwT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Of169jxNvwes"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}