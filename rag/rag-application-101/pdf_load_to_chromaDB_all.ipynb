{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khajum/ai-playground/blob/main/rag/rag-application-101/pdf_load_to_chromaDB_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipelines - Data Ingestion to Verctor DB Pipeline"
      ],
      "metadata": {
        "id": "8kr7TTwnDbCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pypdf if not already installed\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install pypdf\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "f_ra-ZZKDkh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "qu69PJ1gFvQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Read all the PDFs inside directory './data/pdf/'\n",
        "def process_all_pdfs(pdf_directory):\n",
        "  # Process all the pdf files inside the directory\n",
        "  all_documents = []\n",
        "  pdf_dir = Path(pdf_directory)\n",
        "\n",
        "  # Find all the pdf files recursively\n",
        "  pdf_files = list(pdf_dir.glob('**/*.pdf'))\n",
        "\n",
        "  # print all the pdf files found in the directory\n",
        "  print(f\"Found {len(list(pdf_files))} pdf files in {pdf_directory}\")\n",
        "\n",
        "  for pdf_file in pdf_files:\n",
        "    print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "    try:\n",
        "      loader = PyPDFLoader(str(pdf_file))\n",
        "      documents = loader.load()\n",
        "\n",
        "      # Add source information to metadata\n",
        "      for doc in documents:\n",
        "        doc.metadata['source_file'] = str(pdf_file.name)\n",
        "        doc.metadata['file_type'] = \"pdf\"\n",
        "\n",
        "      all_documents.extend(documents)\n",
        "      print(f\" Processed {pdf_file.name} with {len(documents)} pages\")\n",
        "    except Exception as e:\n",
        "      print(f\" Error processing {pdf_file.name}: {e}\")\n",
        "\n",
        "  print(f\" Total documents processed: {len(all_documents)}\")\n",
        "  return all_documents\n",
        "\n",
        "# Process all the PDFs in the data directory\n",
        "# Call the corrected function\n",
        "all_pdf_document = process_all_pdfs(\"./data\")\n",
        "print(f\"Successfully processed and collected {len(all_pdf_document)} documents.\")\n"
      ],
      "metadata": {
        "id": "SMiRQbPtGg6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters"
      ],
      "metadata": {
        "id": "5xShnjMWSw7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_document(documents, chunk_size=1000, chunk_overlap=200):\n",
        "  # split the documents into smaller chucks for better RAG performance\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=chunk_size, chunk_overlap=chunk_overlap,\n",
        "      length_function=len,\n",
        "      separators = [\"\\n\", \"\\n\\n\", \" \", \"\"])\n",
        "  splits = text_splitter.split_documents(documents)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "2fwylAeARpiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all embeding and vectordb related packaged if not already installed\n",
        "!pip install sentence-transformers faiss-cpu chromadb"
      ],
      "metadata": {
        "id": "dhachTpYtNnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "YqUt0HrhtBWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "  \"\"\"Handles the document embeding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "  def __init__(self, embedding_model_name:str=\"all-MiniLM-L6-v2\"):\n",
        "    \"\"\"\n",
        "    Initialize the EmbeddingManager.\n",
        "\n",
        "    Args:\n",
        "      embedding_model_name (str): Name of the embedding model to use.\n",
        "    \"\"\"\n",
        "    self.embedding_model_name = embedding_model_name\n",
        "    self.model = None\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    \"\"\"\n",
        "    Load the SentenceTransformer embedding model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print(f\"Loading SentenceTransformer model: {self.embedding_model_name}\")\n",
        "      self.model = SentenceTransformer(self.embedding_model_name)\n",
        "      print(f\"Loaded SentenceTransformer model successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading embedding model: {e}\")\n",
        "      raise\n",
        "\n",
        "  def generate_embeddings(self, texts:List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "      texts (List[str]): List of texts to generate embeddings for.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray: Array of embeddings.\n",
        "    \"\"\"\n",
        "    if self.model is None:\n",
        "      self._load_model()\n",
        "    try:\n",
        "      print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "      embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "      print(f\"Successfully generated embeddings for {len(texts)} texts with shape: {embeddings.shape}\")\n",
        "      return embeddings\n",
        "    except Exception as e:\n",
        "      print(f\"Error generating embeddings: {e}\")\n",
        "      raise\n",
        "\n",
        "\n",
        "# Initialize the embedding manager\n",
        "embedding_manager = EmbeddingManager()\n",
        "embeddings = embedding_manager.generate_embeddings([doc.page_content for doc in all_pdf_document])\n"
      ],
      "metadata": {
        "id": "nrqO5gAvtTYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "4wI2lje0vdba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import PureWindowsPath\n",
        "from typing import Self, Any\n",
        "import os\n",
        "import chromadb\n",
        "import numpy as np\n",
        "\n",
        "class ChromaVectorStore:\n",
        "  \"\"\" Manages document embeddings into a ChromaDB vector store\"\"\"\n",
        "  def __init__(self, collection_name=\"pdf_documents\", persist_directory=\"./data/vector_store\"):\n",
        "    \"\"\"\n",
        "    Initialize the vector store\n",
        "\n",
        "    Arg:\n",
        "      collection_name: Name of ChromaDB collection\n",
        "      persist_directory: Path to store ChromaDB data\n",
        "    \"\"\"\n",
        "    self.collection_name = collection_name\n",
        "    self.persist_directory = persist_directory\n",
        "    self._initialize_store()\n",
        "\n",
        "  def _initialize_store(self):\n",
        "    \"\"\" Initialize the ChromaDB client and collection\"\"\"\n",
        "    try:\n",
        "      # Create persistent ChromaDB client\n",
        "      os.makedirs(self.persist_directory, exist_ok=True)\n",
        "      self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "      # Get or create collection\n",
        "      self.collection = self.client.get_or_create_collection(\n",
        "          name=self.collection_name,\n",
        "          metadata={\"description\":\"PDF document embeddings for RAG\"})\n",
        "\n",
        "      print(f\"ChromaDB Vector Store initialized with collection: '{self.collection_name}' at path: '{self.persist_directory}'\")\n",
        "      print(f\"Existing documents in the collection: {self.collection.count()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error initializing ChromaDB vector store db: {e}\")\n",
        "      raise\n",
        "\n",
        "  def add_documents(self, documents: list[Any], embeddings: np.ndarray):\n",
        "    \"\"\" Add documents and their embeddings to the vector store\n",
        "\n",
        "    Args:\n",
        "      documents: List of documents to add\n",
        "      embeddings: List of embeddings for the documents\n",
        "    \"\"\"\n",
        "    if len(documents) != len(embeddings):\n",
        "      raise ValueError(\"Number of documents and embeddings must be the same\")\n",
        "\n",
        "    print(f\"Adding {len(documents)} documents to the vector store\")\n",
        "\n",
        "    # Prepare data for Chroma DB\n",
        "    ids = []\n",
        "    metadatas = []\n",
        "    documents_text = []\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i, (document, embedding) in enumerate(zip(documents, embeddings)):\n",
        "      # generate unique Id\n",
        "      document_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "      ids.append(document_id)\n",
        "\n",
        "      # prepare metadata\n",
        "      metadata = dict(document.metadata)\n",
        "      metadata['doc_index'] = i\n",
        "      metadata['content_length'] = len(document.page_content)\n",
        "      metadatas.append(metadata)\n",
        "\n",
        "      # prepare document text\n",
        "      documents_text.append(document.page_content)\n",
        "\n",
        "      # Embedding\n",
        "      embeddings_list.append(embedding.tolist())\n",
        "\n",
        "    # Add data to Chroma DB\n",
        "    self.collection.add(\n",
        "        ids=ids,\n",
        "        metadatas=metadatas,\n",
        "        documents=documents_text,\n",
        "        embeddings=embeddings_list\n",
        "    )\n",
        "    print(f\"Added {len(documents)} documents to the vector store\")\n",
        "    print(f\"Total documents in the collection: {self.collection.count()}\")\n",
        "\n",
        "\n",
        "vector_store = ChromaVectorStore()"
      ],
      "metadata": {
        "id": "v9129M-ovlbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_chunks = split_document(all_pdf_document);\n",
        "print(split_chunks)"
      ],
      "metadata": {
        "id": "2LQHd4LqtRFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text to embeddings\n",
        "texts = [doc.page_content for doc in split_chunks]\n",
        "# generate embeddings\n",
        "embeddings = embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "# store in the vecotor DB\n",
        "vector_store.add_documents(split_chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "GMW4xRymuLTi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}