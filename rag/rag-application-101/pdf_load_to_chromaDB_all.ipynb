{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khajum/ai-playground/blob/main/rag/rag-application-101/pdf_load_to_chromaDB_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipelines - Data Ingestion to Verctor DB Pipeline"
      ],
      "metadata": {
        "id": "8kr7TTwnDbCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup: After required folder is created by below script, upload sample pdf files\n",
        "\n",
        "# Create folder data/pdf under current directory\n",
        "os.makedirs(\"./data/pdf\", exist_ok=True)"
      ],
      "metadata": {
        "id": "-3QcOiYPAqRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pypdf if not already installed\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install pypdf\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "id": "f_ra-ZZKDkh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "qu69PJ1gFvQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Read all the PDFs inside directory './data/pdf/'\n",
        "def process_all_pdfs(pdf_directory):\n",
        "  # Process all the pdf files inside the directory\n",
        "  all_documents = []\n",
        "  pdf_dir = Path(pdf_directory)\n",
        "\n",
        "  # Find all the pdf files recursively\n",
        "  pdf_files = list(pdf_dir.glob('**/*.pdf'))\n",
        "\n",
        "  # print all the pdf files found in the directory\n",
        "  print(f\"Found {len(list(pdf_files))} pdf files in {pdf_directory}\")\n",
        "\n",
        "  for pdf_file in pdf_files:\n",
        "    print(f\"\\nProcessing: {pdf_file.name}\")\n",
        "    try:\n",
        "      loader = PyPDFLoader(str(pdf_file))\n",
        "      documents = loader.load()\n",
        "\n",
        "      # Add source information to metadata\n",
        "      for doc in documents:\n",
        "        doc.metadata['source_file'] = str(pdf_file.name)\n",
        "        doc.metadata['file_type'] = \"pdf\"\n",
        "\n",
        "      all_documents.extend(documents)\n",
        "      print(f\" Processed {pdf_file.name} with {len(documents)} pages\")\n",
        "    except Exception as e:\n",
        "      print(f\" Error processing {pdf_file.name}: {e}\")\n",
        "\n",
        "  print(f\" Total documents processed: {len(all_documents)}\")\n",
        "  return all_documents\n",
        "\n",
        "# Process all the PDFs in the data directory\n",
        "# Call the corrected function\n",
        "all_pdf_document = process_all_pdfs(\"./data\")\n",
        "print(f\"Successfully processed and collected {len(all_pdf_document)} documents.\")\n"
      ],
      "metadata": {
        "id": "SMiRQbPtGg6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters"
      ],
      "metadata": {
        "id": "5xShnjMWSw7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_document(documents, chunk_size=1000, chunk_overlap=200):\n",
        "  # split the documents into smaller chucks for better RAG performance\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=chunk_size, chunk_overlap=chunk_overlap,\n",
        "      length_function=len,\n",
        "      separators = [\"\\n\", \"\\n\\n\", \" \", \"\"])\n",
        "  splits = text_splitter.split_documents(documents)\n",
        "  return splits"
      ],
      "metadata": {
        "id": "2fwylAeARpiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all embeding and vectordb related packaged if not already installed\n",
        "!pip install sentence-transformers faiss-cpu chromadb"
      ],
      "metadata": {
        "id": "dhachTpYtNnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "import uuid\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "YqUt0HrhtBWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "  \"\"\"Handles the document embeding generation using SentenceTransformer\"\"\"\n",
        "\n",
        "  def __init__(self, embedding_model_name:str=\"all-MiniLM-L6-v2\"):\n",
        "    \"\"\"\n",
        "    Initialize the EmbeddingManager.\n",
        "\n",
        "    Args:\n",
        "      embedding_model_name (str): Name of the embedding model to use.\n",
        "    \"\"\"\n",
        "    self.embedding_model_name = embedding_model_name\n",
        "    self.model = None\n",
        "    self._load_model()\n",
        "\n",
        "  def _load_model(self):\n",
        "    \"\"\"\n",
        "    Load the SentenceTransformer embedding model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print(f\"Loading SentenceTransformer model: {self.embedding_model_name}\")\n",
        "      self.model = SentenceTransformer(self.embedding_model_name)\n",
        "      print(f\"Loaded SentenceTransformer model successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
        "    except Exception as e:\n",
        "      print(f\"Error loading embedding model: {e}\")\n",
        "      raise\n",
        "\n",
        "  def generate_embeddings(self, texts:List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "\n",
        "    Args:\n",
        "      texts (List[str]): List of texts to generate embeddings for.\n",
        "\n",
        "    Returns:\n",
        "      np.ndarray: Array of embeddings.\n",
        "    \"\"\"\n",
        "    if self.model is None:\n",
        "      self._load_model()\n",
        "    try:\n",
        "      print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "      embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "      print(f\"Successfully generated embeddings for {len(texts)} texts with shape: {embeddings.shape}\")\n",
        "      return embeddings\n",
        "    except Exception as e:\n",
        "      print(f\"Error generating embeddings: {e}\")\n",
        "      raise\n",
        "\n",
        "\n",
        "# Initialize the embedding manager\n",
        "embedding_manager = EmbeddingManager()\n",
        "embeddings = embedding_manager.generate_embeddings([doc.page_content for doc in all_pdf_document])\n"
      ],
      "metadata": {
        "id": "nrqO5gAvtTYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "4wI2lje0vdba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import PureWindowsPath\n",
        "from typing import Self, Any\n",
        "import os\n",
        "import chromadb\n",
        "import numpy as np\n",
        "\n",
        "class ChromaVectorStore:\n",
        "  \"\"\" Manages document embeddings into a ChromaDB vector store\"\"\"\n",
        "  def __init__(self, collection_name=\"pdf_documents\", persist_directory=\"./data/vector_store\"):\n",
        "    \"\"\"\n",
        "    Initialize the vector store\n",
        "\n",
        "    Arg:\n",
        "      collection_name: Name of ChromaDB collection\n",
        "      persist_directory: Path to store ChromaDB data\n",
        "    \"\"\"\n",
        "    self.collection_name = collection_name\n",
        "    self.persist_directory = persist_directory\n",
        "    self._initialize_store()\n",
        "\n",
        "  def _initialize_store(self):\n",
        "    \"\"\" Initialize the ChromaDB client and collection\"\"\"\n",
        "    try:\n",
        "      # Create persistent ChromaDB client\n",
        "      os.makedirs(self.persist_directory, exist_ok=True)\n",
        "      self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
        "\n",
        "      # Get or create collection\n",
        "      self.collection = self.client.get_or_create_collection(\n",
        "          name=self.collection_name,\n",
        "          metadata={\"description\":\"PDF document embeddings for RAG\"})\n",
        "\n",
        "      print(f\"ChromaDB Vector Store initialized with collection: '{self.collection_name}' at path: '{self.persist_directory}'\")\n",
        "      print(f\"Existing documents in the collection: {self.collection.count()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error initializing ChromaDB vector store db: {e}\")\n",
        "      raise\n",
        "\n",
        "  def add_documents(self, documents: list[Any], embeddings: np.ndarray):\n",
        "    \"\"\" Add documents and their embeddings to the vector store\n",
        "\n",
        "    Args:\n",
        "      documents: List of documents to add\n",
        "      embeddings: List of embeddings for the documents\n",
        "    \"\"\"\n",
        "    if len(documents) != len(embeddings):\n",
        "      raise ValueError(\"Number of documents and embeddings must be the same\")\n",
        "\n",
        "    print(f\"Adding {len(documents)} documents to the vector store\")\n",
        "\n",
        "    # Prepare data for Chroma DB\n",
        "    ids = []\n",
        "    metadatas = []\n",
        "    documents_text = []\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i, (document, embedding) in enumerate(zip(documents, embeddings)):\n",
        "      # generate unique Id\n",
        "      document_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
        "      ids.append(document_id)\n",
        "\n",
        "      # prepare metadata\n",
        "      metadata = dict(document.metadata)\n",
        "      metadata['doc_index'] = i\n",
        "      metadata['content_length'] = len(document.page_content)\n",
        "      metadatas.append(metadata)\n",
        "\n",
        "      # prepare document text\n",
        "      documents_text.append(document.page_content)\n",
        "\n",
        "      # Embedding\n",
        "      embeddings_list.append(embedding.tolist())\n",
        "\n",
        "    # Add data to Chroma DB\n",
        "    self.collection.add(\n",
        "        ids=ids,\n",
        "        metadatas=metadatas,\n",
        "        documents=documents_text,\n",
        "        embeddings=embeddings_list\n",
        "    )\n",
        "    print(f\"Added {len(documents)} documents to the vector store\")\n",
        "    print(f\"Total documents in the collection: {self.collection.count()}\")\n",
        "\n",
        "\n",
        "vector_store = ChromaVectorStore()"
      ],
      "metadata": {
        "id": "v9129M-ovlbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_chunks = split_document(all_pdf_document);\n",
        "print(split_chunks)"
      ],
      "metadata": {
        "id": "2LQHd4LqtRFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the text to embeddings\n",
        "texts = [doc.page_content for doc in split_chunks]\n",
        "# generate embeddings\n",
        "embeddings = embedding_manager.generate_embeddings(texts)\n",
        "\n",
        "# store in the vecotor DB\n",
        "vector_store.add_documents(split_chunks, embeddings)\n"
      ],
      "metadata": {
        "id": "GMW4xRymuLTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGRetreiver:\n",
        "  \"\"\" handles query-based retrieval from the vector store\"\"\"\n",
        "\n",
        "  def __init__(self, vector_store: ChromaVectorStore, embedding_manager: EmbeddingManager):\n",
        "    \"\"\"\n",
        "    Initialize the RAG retriever\n",
        "\n",
        "    Args:\n",
        "      vector_store: ChromaDB vector store\n",
        "      embedding_manager: Embedding manager\n",
        "    \"\"\"\n",
        "    self.vector_store = vector_store\n",
        "    self.embedding_manager = embedding_manager\n",
        "\n",
        "  def retrieve(self, query: str, top_k: int = 5, score_threshold: float=0.0) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Retrieve documents based on a query\n",
        "\n",
        "    Args:\n",
        "      query: Query string\n",
        "      top_k: Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "      List of retrieved documents\n",
        "    \"\"\"\n",
        "    print(f\"Retrieving documents for query: '{query}'\")\n",
        "    print(f\"Top K: {top_k}, Score Threshold: {score_threshold}\")\n",
        "    try:\n",
        "      # Generate embeddings for the query\n",
        "      query_embedding = self.embedding_manager.generate_embeddings([query])\n",
        "      #print(f\"Query embedding generated: {query_embedding}\")\n",
        "\n",
        "      # search in vector store\n",
        "      print(f\"Searching for documents in vector store...\")\n",
        "      results = self.vector_store.collection.query(\n",
        "          query_embeddings=query_embedding.tolist(),\n",
        "          n_results=top_k\n",
        "\n",
        "      )\n",
        "      print(f\"Found {len(results['documents'])} documents in the vector store\")\n",
        "      #print(f\"Results: {results}\")\n",
        "\n",
        "      #process results\n",
        "      retrieved_documents = []\n",
        "\n",
        "      for i, document in enumerate(results['documents'][0]):\n",
        "\n",
        "        score = results['distances'][0][i]\n",
        "        if score < score_threshold:\n",
        "          continue\n",
        "        metadata = results['metadatas'][0][i]\n",
        "        ids = results['ids'][0][i]\n",
        "\n",
        "        retrieved_documents.append({\n",
        "            'id': ids,\n",
        "            'page_content': document,\n",
        "            'metadata': metadata,\n",
        "            'similarity_score': 1 - score,\n",
        "            'distance': score,\n",
        "            'rank': i + 1\n",
        "        })\n",
        "      print(f\"Successfully retrieved {len(retrieved_documents)} documents\")\n",
        "      return retrieved_documents\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error retrieving documents: {e}\")\n",
        "      raise"
      ],
      "metadata": {
        "id": "-qx6DXfDzIEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test RAG Retreiver for a given query\n",
        "rag_retriever = RAGRetreiver(vector_store, embedding_manager)\n",
        "result = rag_retriever.retrieve(\"what is attention?\")\n",
        "print(\"Retrive document result:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "95QZTwLb5l01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple RAG Pipeline with Groq LLM Output"
      ],
      "metadata": {
        "id": "mYTKuaM-LbBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq python-dotenv"
      ],
      "metadata": {
        "id": "pvsN8MPWLnRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Simple RAG pipeline with Groq LLM\n",
        "from langchain_groq import ChatGroq\n",
        "import os\n",
        "# from dotenv import load_dotenv # Commented out, as we'll use Colab's secrets manager\n",
        "from google.colab import userdata # Import userdata for Colab secrets\n",
        "\n",
        "# load_dotenv() # Commented out\n",
        "\n",
        "# Get the GROQ LLM API key from Colab's Secrets Manager\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "if not groq_api_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found in Colab Secrets. Please set it there.\")\n",
        "\n",
        "print(\"GROQ_API_KEY loaded successfully from Colab Secrets.\")\n",
        "\n",
        "# Initialize the GRQ LLM with a supported model\n",
        "llm = ChatGroq(model_name=\"openai/gpt-oss-120b\", groq_api_key=groq_api_key, temperature=0.1)\n",
        "\n",
        "# Simple RAG function: retrieval context + generate response\n",
        "def rag_pipeline(query: str, retriver, llm, top_k: int = 5):\n",
        "  print(f\"RAG pipeline with query: '{query}'\")\n",
        "\n",
        "  # retrieve the context\n",
        "  results = retriver.retrieve(query, top_k=top_k)\n",
        "\n",
        "  context = \"\\n\".join([doc['page_content'] for doc in results])\n",
        "  if not context:\n",
        "    return \"No relevant context found to answer the question\"\n",
        "\n",
        "  # generate response using GROQ LLM\n",
        "  prompt=f\"\"\"Use the following pieces of context to answer the question concisely.\n",
        "  Context:\n",
        "  {context}\n",
        "  Question: {query}\n",
        "  Answer:\"\"\"\n",
        "  response = llm.invoke([prompt.format(context=context, question = query)])\n",
        "  return response.content\n"
      ],
      "metadata": {
        "id": "lzWZORs2MAJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rag_pipeline(\"What is Attention?\", rag_retriever, llm)\n",
        "#answer = rag_pipeline(\"Limitation of Attention?\", rag_retriever, llm)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "vGt-dBZtdj_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhance RAG Pipeline"
      ],
      "metadata": {
        "id": "Z0PDLO8iopgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced RAG Pipeline Feature\n",
        "def rag_advanced_pipeline(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
        "  \"\"\"\n",
        "  RAG pipeline with advanced features:\n",
        "  - Returns answer, sources, confidence score and optionally full context\n",
        "  - Context filtering based on similarity score\n",
        "  \"\"\"\n",
        "  results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
        "  if not results:\n",
        "    return {'answer':'No relevant context found to answer the question', 'source': [], 'confidence': 0.0, 'context': ''}\n",
        "\n",
        "  # prepare the context and source\n",
        "  context = \"\\n\".join([doc['page_content'] for doc in results])\n",
        "  sources = [{\n",
        "      'source': doc['metadata']['source_file'],\n",
        "      'page': doc['metadata'].get('page', 'Unknown'),\n",
        "      'similarity_score': doc['similarity_score'],\n",
        "      'distance': doc['distance'],\n",
        "      'rank': doc['rank'],\n",
        "      'preview': doc['page_content'][:200] + '...'\n",
        "  } for doc in results]\n",
        "  confidence = max([doc['similarity_score'] for doc in results])\n",
        "\n",
        "  # generate response using GROQ LLM\n",
        "  prompt=f\"\"\"Use the following pieces of context to answer the question concisely.\n",
        "  Context:\n",
        "  {context}\n",
        "  Question: {query}\n",
        "  Answer:\"\"\"\n",
        "  response = llm.invoke([prompt.format(context=context, question = query)])\n",
        "  answer = response.content\n",
        "\n",
        "  if return_context:\n",
        "    return {'answer': answer, 'source': sources, 'confidence': confidence, 'context': context}\n",
        "  else:\n",
        "    return {'answer': answer, 'source': sources, 'confidence': confidence}\n",
        "\n"
      ],
      "metadata": {
        "id": "9YZEsG38ozlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "#query = \"What is Attention?\"\n",
        "query = \"Overview of Attention Mechanism Architecture?\"\n",
        "#query = \"What is Petrol Engine?\"\n",
        "result = rag_advanced_pipeline(query, rag_retriever, llm, top_k=5, min_score=0.1, return_context=True)\n",
        "print(\"Answer:\", result['answer'])\n",
        "print(\"Sources:\", result['source'])\n",
        "print(\"Confidence:\", result['confidence'])\n",
        "print(\"Context Preview:\", result['context'][:200])"
      ],
      "metadata": {
        "id": "juaVtym3tmZO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}