# === Core scientific stack ===
numpy>=1.23
pandas>=1.5
scikit-learn>=1.2
matplotlib>=3.7
seaborn>=0.12

# === Jupyter / notebooks ===
jupyter>=1.0

# === PyTorch (install variant depends on your machine) ===
# For CPU-only via pip, uncomment the next line:
# torch>=2.1.0
# torchvision>=0.16.0
# NOTE: For NVIDIA GPUs (CUDA), install torch/torchvision from https://pytorch.org/get-started/locally/
# (do NOT pin CUDA wheels here; follow PyTorch's install command)

# === Hugging Face ecosystem ===
transformers>=4.37
datasets>=2.18
accelerate>=0.27
peft>=0.8
trl>=0.7
sentencepiece>=0.1.99
safetensors>=0.4
huggingface-hub>=0.21
evaluate>=0.4

# === RAG / Vector search (optional) ===
# Choose ONE of the following depending on your platform:
faiss-cpu>=1.7.4
# faiss-gpu>=1.7.4  # if building from source or conda with CUDA support

# === Quantization / efficient inference (optional) ===
# bitsandbytes requires compatible CUDA; see its docs.
# Comment out if you only run CPU.
bitsandbytes>=0.43
# auto-gptq is optional for GPTQ quantized models
# auto-gptq>=0.7

# === API & serving ===
fastapi>=0.110
uvicorn[standard]>=0.29
pydantic>=2.6
python-dotenv>=1.0

# === Dev tools / quality / tracking ===
black>=24.3
ruff>=0.4
pytest>=8.0
mlflow>=2.10
# Alternatively: wandb>=0.16
