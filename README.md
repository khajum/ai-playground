
# ğŸ§  AI Engineering Roadmap & LLM Lab

> A comprehensive, hands-on learning and building repo for AI Engineeringâ€”covering ML fundamentals, Deep Learning, Transformers, LLMs, Prompt Engineering, Paper Implementations, Experiments, and Capstone Projects.
>
> By: **Ram Limbu**

[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](#license)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Frameworks](https://img.shields.io/badge/Frameworks-PyTorch%20%7C%20TensorFlow%20%7C%20HF%20Transformers-orange.svg)](https://pytorch.org/)

---

## ğŸ“š Whatâ€™s Inside
- **Roadmap** â€“ A structured path from fundamentals â†’ DL â†’ LLMs â†’ production.
- **Exercises & Solutions** â€“ Progressive, hands-on practice across ML/AI topics.
- **LLM Lab** â€“ Prompt engineering, finetuning, evaluation, deployment.
- **Paper Implementations** â€“ Reproducible, annotated implementations of key papers.
- **Capstone Projects** â€“ End-to-end projects showcasing real-world problem solving.
- **MLOps & Systems** â€“ Data pipelines, evaluation, monitoring, and packaging.

---

## ğŸš€ Quick Start

### 1) Create Environment
```bash
# Using conda
conda env create -f environment.yml
conda activate ai-roadmap

# Or using pip
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 2) Run a Demo
```bash
# Train a simple classifier (example)
make train            # or: python scripts/train.py --config configs/ml_basics.yaml

# Evaluate
make evaluate         # or: python scripts/evaluate.py --task llm_eval

# Serve an LLM endpoint
make serve            # or: uvicorn scripts.serve:app --reload
```

> Tip: See **[`docs/roadmap.md`](docs/roadmap.md)** for the full learning plan, deliverables, and checklists.

---

## ğŸ—‚ Repository Structure
```text
ai-playground /
â”œâ”€ README.md
â”œâ”€ .gitignore
â”œâ”€ environment.yml
â”œâ”€ requirements.txt
â”œâ”€ Makefile
â”œâ”€ docs/
â”‚  â”œâ”€ roadmap.md
â”‚  â”œâ”€ references.md
â”‚  â””â”€ architecture-notes/
â”œâ”€ notebooks/
â”‚  â”œâ”€ 01_ml_basics.ipynb
â”‚  â”œâ”€ 02_deep_learning.ipynb
â”‚  â”œâ”€ 03_transformers.ipynb
â”‚  â””â”€ 04_llm_eval.ipynb
â”œâ”€ exercises/
â”‚  â”œâ”€ ml-from-scratch/
â”‚  â”œâ”€ deep-learning-playground/
â”‚  â””â”€ reinforcement-learning-gym/
â”œâ”€ llm-lab/
â”‚  â”œâ”€ prompt-engineering-lab/
â”‚  â”œâ”€ finetune/
â”‚  â”œâ”€ inference/
â”‚  â”œâ”€ evaluation/
â”‚  â””â”€ deployment/
â”œâ”€ paper-implementations/
â”‚  â”œâ”€ attention-is-all-you-need/
â”‚  â”œâ”€ diffusion-basics/
â”‚  â””â”€ retrieval-augmented-generation/
â”œâ”€ capstones/
â”‚  â”œâ”€ llm-finetuning-hub/
â”‚  â”œâ”€ generative-ai-showcase/
â”‚  â””â”€ ai-capstone-projects/
â”œâ”€ scripts/
â”‚  â”œâ”€ setup_env.sh
â”‚  â”œâ”€ train.py
â”‚  â”œâ”€ evaluate.py
â”‚  â””â”€ serve.py
â””â”€ tests/
```

---

## ğŸ§­ Learn by Building: Roadmap (Phases)
The full roadmap lives in **[`docs/roadmap.md`](docs/roadmap.md)**. Highlights:
- **Phase 0 â€” Foundations:** Python, math for ML, classic algorithms.
- **Phase 1 â€” Deep Learning:** PyTorch, CNN/RNN, experiment tracking.
- **Phase 2 â€” Transformers:** Attention, tokenization, paper implementation.
- **Phase 3 â€” LLMs in Practice:** HF ecosystem, prompts, RAG, evaluation.
- **Phase 4 â€” Finetuning & Optimization:** LoRA/QLoRA, quantization, inference.
- **Phase 5 â€” Production AI:** APIs, CI/CD, Docker, monitoring, Responsible AI.

---

## ğŸ§ª Modules & Demos
- `llm-lab/prompt-engineering-lab/` â€“ Prompt patterns (Zero/Few-shot, CoT, ReAct) + eval harness.
- `llm-lab/finetune/` â€“ SFT with PEFT (LoRA/QLoRA), configs, scripts.
- `llm-lab/inference/` â€“ Efficient CPU/GPU pipelines, quantized inference.
- `llm-lab/evaluation/` â€“ Metrics: accuracy, ROUGE/BLEU, toxicity, hallucinations.
- `llm-lab/deployment/` â€“ FastAPI service, Dockerfiles, `compose.yml`.

---

## âš™ï¸ Makefile Commands
```make
.PHONY: setup train evaluate serve lint test

setup:
	python -m venv .venv && . .venv/bin/activate && pip install -r requirements.txt

train:
	python scripts/train.py --config configs/train.yaml

evaluate:
	python scripts/evaluate.py --task llm_eval

serve:
	uvicorn scripts.serve:app --host 0.0.0.0 --port 8000 --reload

lint:
	ruff . && black --check .

test:
	pytest -q
```

---

## ğŸ” Responsible AI
- Document intended use, limitations, and safety controls (model cards).
- Evaluate toxicity, bias, and hallucination rates; add guardrails.
- Respect privacy, compliance, and safe failure modes.

---

## ğŸ¤ Contributing
1. Fork & create a feature branch: `feat/<module-name>`
2. Add tests and docs for new modules
3. Run `make lint && make test`
4. Open a PR with a clear description, screenshots, and benchmarks

---

## ğŸ“¦ License
MIT â€” see [`LICENSE`](./LICENSE).

---

## ğŸ™Œ Acknowledgements
This repo leverages the open-source ecosystem (PyTorch, Hugging Face Transformers/Datasets, PEFT/TRL, FastAPI) and community best practices.

---

*By: Ram Limbu â€” Last updated: 2025-12-27*
